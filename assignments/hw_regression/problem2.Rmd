---
title: 'Multiple Regression. Problem 2.'
author: "<your name here>"
date: "07 05 2021"
output: 
  html_document: 
    df_print: kable
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Preparation

```{r, message=F, warning=F}
library(car)
library(GGally)
library(ggfortify)
library(modelr)
library(tidyverse)
library(glmnet)
```



# Problem 2. Predicting the per capita crime rate in the Boston data set (3 points)

Let's load the `Boston` dataset which is available in the `MASS` package.


```{r}
library(MASS)
data(Boston)
head(Boston)
```

The data contains the following characteristics of 506 city zones (corresponding to census tract codes):

  - `crim` - per capita crime rate by town.  
  - `zn`- proportion of residential land zoned for lots over 25,000 sq.ft.  
  - `indus` - proportion of non-retail business acres per town.  
  - `chas` - Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).  
  - `nox` - nitrogen oxides concentration (parts per 10 million).  
  - `rm` - average number of rooms per dwelling.  
  - `age` - proportion of owner-occupied units built prior to 1940.  
  - `dis` - weighted mean of distances to five Boston employment centres.  
  - `rad` - index of accessibility to radial highways.  
  - `tax` - full-value property-tax rate per \$10,000.  
  - `ptratio` - pupil-teacher ratio by town.  
  - `black` - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.  
  - `lstat` - lower status of the population (percent).
  - `medv` - median value of owner-occupied homes in \$1000s.  
  
In this assignment you'll create a model to predict the per capita crime rate `crim`.

Let's split the dataset into training and test data.

```{r}
set.seed(12345)
test_ids <- sample.int(nrow(Boston), size = nrow(Boston) * 0.2)
boston_test <- Boston[test_ids, ]
boston_train <- Boston[-test_ids, ]
```


## Task 1.1 Explore pairwise correlations

Produce a scatterplot matrix using the `ggpairs()` function with default parameters. Note, that `chas` is a categorical variable, so it's best to convert it to a factor for this analysis to get a suitable plot.

The number of variables is quite large, so you might need to split the variables into two subsets (both containing the target variable), to make the visualization more interpretable. 

```{r, message=F, warning=F}

# Your code here

```


The ggpairs() will produce both graphical output and the numerical values for pairwise correlations between all numerical variables. For categorical data only the visualizations will be provided, since the correlation coefficient can't be computed for it.

Write-up your interpretation of the results:
- Which variables are correlated to the target variable?
- What is the strongest predictor?
- Are there any non-linear relationships?
- What predictors are highly correlated?
- Is the categorical variable (`chas`) correlated to the target variable?


## Task 1.2 The linear model with all predictors

Produce a linear model for `crim` with all predictors. Use the training dataset (`boston_train`) to build the model. Store the model in the `m1` variable.

```{r}
# Your code here

```

Write your interpretation of the model summary:
- Is there a relationship between the predictors and the response (according to the F-statistic)?
- Which predictors appear to have a statistically significant relationship to the response?


## Task 1.3 Check model assumptions

Use the `autoplot()` function to produce diagnostic plots of the linear regression fit.
You may find the 1, 2 3 and 6 plots the most informative. To produce them, use: c(1, 2, 3, 6) as a second argument for the `autoplot()` function.


```{r}
# Your code here

```


Comment on any problems you see with the fit. 

Build a second model `m2` also with all predictors. This time use `log(crim)` instead of `crim` in the model specification. Compare the diagnostic plots for the second model. Are there any improvements?

```{r}

# Your code here

```


## Task 1.4 Stepwise regression

Use `stepAIC()` function to produce a third model `m3` based on `m2`. Use 'stepwise stepwise' method (`direction = 'both'`).


```{r}

# Your code here

```

Interpret the summary for `m3`. Are some predictors removed from the model? How does the new model compare to `m2`?

Are there any improvements in the diagnostic plots for `m3`?

```{r}
# Your code here
```


## Task 1.5. Lasso regression

Now use another feature selection algorithm based on the Regularization principle - Lasso regression.
You'll need to prepare the target vector `y` and the predictor matrix `X` in order to run `glmnet()`.
It's best also to standardize the predictors to make the contribution of each predictor to the penalty term similar.

```{r}
y <- log(boston_train$crim)
X = boston_train %>% dplyr::select(-crim) %>% as.matrix() %>% scale()
```

First, find the optimized value for $\lambda$. Remember that for Lasso regression $\alpha = 1$. Use `cv.glmnet()` for this.

```{r}

# Your code here

```

What value of $\lambda$ produces the least CV error?


Produce a plot displaying how the coefficients did change with increase in $\lambda$. What are the two most important predictors according to the plot? (The numbers on the plot correspond to the columns in the predictor matrix `X`). 


```{r}

# Your code here

```


What are the coefficients for the Lasso model?

```{r}

# Your code here

```




## Task 1.6 Comparing the models on the test data

Now we'll compare the performance of the models on the new, unseen data. We'll use the `ME`, `RMSE`, `MAE` and `MAPE` metrics for this. 

Since we've log-transformed the target variable, we can't use the functions defined in `modelr()`. Instead we'll define our custom function for this. This function will take the actual and the predicted values and return all the error metrics.

```{r}
errors <- function(actual, predicted) {
  error <- actual - predicted
  abs_error <- abs(actual - predicted)
  percent_error <- abs_error/actual
  
  ME <- mean(error, na.rm = T)
  MAE <- mean(abs_error, na.rm = T)
  MAPE <- mean(percent_error, na.rm = T)
  MSE <- mean(error^2, na.rm = T)
  RMSE <- sqrt(MSE)
  
  tibble(ME, MAE, MAPE, RMSE)
}
```

First, we calculate predictions from the 'm2' model:

```{r}
pred_m2 <-
  boston_test %>%
  add_predictions(m2, var='log_predicted') %>%
  mutate(predicted = exp(log_predicted)) %>%
  dplyr::select(crim, predicted)

head(pred_m2)
```

```{r}
pred_m2 %>%
  ggplot(aes(predicted, crim)) + geom_point() + geom_abline(color='red') + 
  labs(title = 'Predicted vs actual per capita crime rate, Model 2')
```

Let's compute errors for the `m2` model using the previously defined function:

```{r}
errors(pred_m2$crim, pred_m2$predicted)
```


Now, compute predictions for the `m3` model, similarly to the previous example:

```{r}

# Your code here

```

Produce the predicted - actual plot similar to the previous one for the `m3` model:

```{r}

# Your code here

```


Finally, we compute predictions for the Lasso model.

```{r}
new_X <- boston_test %>% dplyr::select(-crim) %>% as.matrix() %>% scale()

pred_log <- predict(cv_lasso, newx = new_X, s = cv_lasso$lambda.min)

pred_m4 <- tibble(crim = boston_test$crim, 
                  predicted = exp(pred_log))

```

Produce the predicted - actual plot similar to the previous one for the `m4` model:

```{r}

# Your code here

```

Next, let's combine all results into a single table


```{r}
# Combine all the prediction tables into one large able
all_models <- 
  bind_rows('m2: All predictors' = pred_m2, 
            'm3: Stepwise' = pred_m3, 
            'm4: Lasso' = pred_m4,
            .id = "model")

# Compute model performance metrics
all_models %>%
  group_by(model) %>% # group by model name 
  summarize(errors=list(errors(crim, predicted))) %>% # calculate errors for each group
  unnest() %>% # unwrap the resulting table with metrics into separate columns
  arrange(MAPE) # sort by increasing MAPE
```


Interpret the results. Which model has the least bias (in terms of Mean Error)? Which model is the best in terms of MAPE?


Finally, compute the error metrics for all three models on the training data. Which model has the least change in MAPE between the training and the test set?

```{r}

# Your code here

```



## Task 1.7 Improve the model performance (for up to 3 extra points)

All of our models were quite poor in terms of performance. The best model had ~64% MAPE on the test set. Can you improve the models? Try a few approaches, for example 
- filter out the influential observations from the training set, 
- include some interactions or polynomial terms, 
- try different variable transformations,
- try a different approach to predictor selection.

Make sure to use only the training data to build your model. Then compare its performance to the previously built models (`m2`-`m4`) on the test data.





