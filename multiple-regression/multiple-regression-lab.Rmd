---
title: "Lab on Multiple Regression [Adapted from ISLR book, Chapter 3]"
author: "Zakhodyakin Gleb"
date: "12 09 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Packages

The `library()` function is used to load *packages*, or groups of functions and
data sets that are not included in the base R distribution. Basic functions
that perform least squares linear regression and other simple analyses come
standard with the base distribution, but more exotic functions require additional
libraries. Here we load the `MASS` package, which is a very large collection of data sets and functions. We also load the `ISLR` package, which includes the data sets associated with the ISLR book. The `tidyverse` package contains a powerful set of tools for loading, transforming and visualizing data using an unified syntax.


```{r, message=F, warning=F}
library(MASS)
library(ISLR)
library(ggfortify) # to visualize the diagnostic plots using ggplot
library(GGally) # Scatterplot Matrix, the ggplot way
library(car) # to compute VIF
library(tidyverse)
library(modelr) # tidy modeling tools

```

If you receive an error message when loading any of these packages, it
likely indicates that the corresponding package has not yet been installed
on your system. Some packages, such as `MASS`, come with R and do not need to
be separately installed on your computer. However, other packages, such as
`ISLR`, must be downloaded the first time they are used. This can be done directly
from within RStudio. Select `Tools/Install Packages` menu for this. 

Simply type the name of a package you wish to install and RStudio will automatically download the package. Alternatively, this can be done at the R command line via `install.packages("ISLR")`. This installation only needs to be done the first time you use a package. However, the `library()` function must be called each time you wish to use a given package in a new script or R Markdown document.


# Simple linear regression

The `MASS` package contains the Boston data set, which records `medv` (median
house value) for 506 neighborhoods around Boston. We will seek to predict
`medv` using 13 predictors such as `rm` (average number of rooms per house),
`age` (average age of houses), and `lstat` (percent of households with low
socioeconomic status).


You can have a quick look at the Boston data set:

```{r}
data(Boston)
head(Boston)
```


To quickly view names of the variables in the dataset, use `names()`:

```{r}
names(Boston)
```

To find out more on the Boston data set, see help: `?Boston`

```{r}
# Type here ?Boston and run the chunk
```


We will start by using the `lm()` function to fit a simple linear regression
`lm()` model, with `medv` as the response and `lstat` as the predictor. The basic
syntax is `lm(y ∼ x, data)`, where `y` is the response, `x` is the predictor, and
`data` is the data set in which these two variables are kept.


```{r}
lm.fit  <-  lm(medv ~ lstat, data = Boston)
```

The model is saved into 'lm.fit` variable. 

If we type `lm.fit`, some basic information about the model is output.
For more detailed information, we use `summary(lm.fit)`. This gives us p-values
and standard errors for the coefficients, as well as the $R^2$ statistic
and F-statistic for the model.

```{r}
lm.fit
```


```{r}

summary(lm.fit)

```

We can use the `names()` function in order to find out what other pieces
of information are stored in `lm.fit`. Although we can extract these quantities
by name—e.g. `lm.fit$coefficients`—it is safer to use the extractor
functions like `coef()` to access them.

```{r}
names(lm.fit)
```

```{r}
coef(lm.fit)
```

In order to obtain a confidence interval for the coefficient estimates, we can
use the `confint()` command

```{r}
confint(lm.fit)
```

The `predict()` function can be used to produce confidence intervals and prediction intervals for the prediction of `medv` for a given value of `lstat`.


```{r}
# Creating a new data set:
new_data_lstat <- tibble(lstat = seq(5, 25, by = 5))

new_data_lstat


```

Notice, that the new data set should include columns for all the predictors used in the model.

For more complex datasets, consider using the `dplyr::tribble()` function to enter them manually, or edit them in Excel and then import into RStudio.

```{r}
# Creating predictions - a point forecast
predict(lm.fit, newdata = new_data_lstat)
```

The result is a vector, which can be stored in the data set used for predictions:

```{r}
cbind(new_data_lstat, 
      fit = predict(lm.fit, newdata = new_data_lstat))
```

Or you can use `modelr::add_predictions()` function:

```{r}
add_predictions(new_data_lstat, lm.fit, var = 'fit')
  
```


To compute confidence or prediction intervals, use the `interval=` argument:

```{r}
pred_int <- predict(lm.fit, newdata = new_data_lstat, 
                    interval = 'prediction')

cbind(new_data_lstat, pi = pred_int)
```

```{r}
conf_int <- predict(lm.fit, newdata = new_data_lstat, 
                    interval = 'confidence')

cbind(new_data_lstat, ci = conf_int)

```


For instance, the 95% confidence interval associated with a lstat value of 10 is (24.47, 25.63), and the 95% prediction interval is (12.828, 37.28). As expected, the confidence and prediction intervals are centered around the same point (a predicted value of 25.05 for medv when lstat equals 10), but the latter are substantially wider. 

We will now plot `medv` and `lstat` along with the least squares regression line.

To plot this line, we could use the `geom_smooth(method = 'lm')` layer, or the `geom_abline()` layer, or could simply compute some points for this line using training or new data.
Here we'll use the latter aproach.

```{r}

Boston %>%
  add_predictions(model = lm.fit, var = 'fit') %>%
  ggplot() +
  geom_point(aes(x = lstat, y = medv)) + 
  geom_line(aes(x = lstat, y = fit), color = 'blue')
```

We can also plot the regression line together with prediction and confidence intervals:


```{r}

ci_Boston <- predict(lm.fit, interval = 'confidence')
pi_Boston <- predict(lm.fit, interval = 'prediction')

cbind(Boston, ci = ci_Boston, pi = pi_Boston) %>%
  ggplot(aes(x = lstat, y = medv)) +
  geom_ribbon(aes(ymin = pi.lwr, ymax = pi.upr), fill = 'lightskyblue') +
  geom_ribbon(aes(ymin = ci.lwr, ymax = ci.upr), fill = 'darkgray') +
  geom_line(aes(y = ci.fit), color = 'red') +
  geom_point()

```


Note that the prediction interval is much wider than the confidence interval, since it includes the uncertainty associated with large residuals of the model.



Next we examine some diagnostic plots, several of which were discussed in Section 3.3.3. Four diagnostic plots are automatically produced by applying the `ggfortify::autoplot()` function directly to the output from `lm()`.


```{r}
autoplot(lm.fit)
```


You can control which plots to display, using the 

```{r}
autoplot(lm.fit, which = c(1, 3))
```

Alternatively, we can compute the residuals from a linear regression fit using the `residuals()` function. The function `rstudent()` will return the studentized residuals, and we can use this function to plot the residuals against the fitted values.


```{r}
qplot(fitted(lm.fit), residuals(lm.fit),
      main = 'Residuals vs Fitted')

qplot(fitted(lm.fit), rstudent(lm.fit),
      main = 'Studentized Residuals vs Fitted')
```


On the basis of the residual plots, there is some evidence of non-linearity.
Leverage statistics can be computed for any number of predictors using the
`hatvalues()` function.


```{r}
qplot(hatvalues(lm.fit))
```



```{r}
which.max(hatvalues(lm.fit))
```

The `which.max()` function identifies the index of the largest element of a `which.max()`
vector. In this case, it tells us which observation has the largest leverage statistic.

Note that it's generally more useful not only find the largest leverage index, but also to display the associated observation. We can achieve this easily using functions for data transformation available in the `dplyr` package:


```{r}
Boston %>%
  mutate(leverage = hatvalues(lm.fit)) %>% #adding leverage statistic to the dataset
  select(leverage, everything()) %>% # making leverage the first column
  top_n(3, wt = leverage)  %>% # selecting 3 observations with largest leverage
  arrange(desc(leverage)) # ordering by leverage
```

Note also that the leverage statistic is present on the diagnostic plots.

```{r}
autoplot(lm.fit, which = 5)
```


# Multiple Linear Regression


In order to fit a multiple linear regression model using least squares, we again use the `lm()` function. The syntax `lm(y∼x1+x2+x3)` is used to fit a model with three predictors, `x1`, `x2`, and `x3`. The `summary()` function now outputs the regression coefficients for all the predictors.


```{r}
lm.fit2 <- lm(medv ~ lstat + age, data = Boston)

summary(lm.fit2)
```

The Boston data set contains 13 variables, and so it would be cumbersome to have to type all of these in order to perform a regression using all of the predictors. Instead, we can use the following short-hand:

```{r}
lm.fit.all <- lm(medv ~ ., data = Boston)
summary(lm.fit.all)
```


We can access the individual components of a summary object by name (type `?summary.lm` to see what is available). Hence `summary(lm.fit)$r.sq` gives us the R2, and `summary(lm.fit)$sigma` gives us the RSE. 

The `vif()` function, part of the car package, can be used to compute variance inflation
factors. Most VIF’s are low to moderate for this data. The `car` package is not part of the base R installation so it must be downloaded the first time you use it via the `install.packages` option in R.

```{r}
vif(lm.fit.all) %>% 
  round(2)
```


What if we would like to perform a regression using all of the variables but one? For example, in the above regression output, `age` has a high p-value. So we may wish to run a regression excluding this predictor. The following syntax results in a regression using all predictors except `age`.

```{r}
lm.fit.all_but_age <- 
  lm(medv ~ . -age, data = Boston)

summary(lm.fit.all_but_age)
```


Alternatively, the `update()` function can be used.

```{r}
update(lm.fit.all, ~ . -age) %>%
  summary()
```

# Interaction Terms

It is easy to include interaction terms in a linear model using the `lm()` function.
The syntax `lstat:black` tells R to include an interaction term between `lstat` and `black`. The syntax `lstat*age` simultaneously includes `lstat`, `age`, and the interaction term `lstat × age` as predictors; it is a shorthand for `lstat + age + lstat:age`.

```{r}
lm(medv ~ lstat * age, data = Boston) %>%
  summary()
```

# Non-linear Transformations of the Predictors

The `lm()` function can also accommodate non-linear transformations of the predictors. For instance, given a predictor `X`, we can create a predictor `X^2` using `I(X^2)`. The function `I()` is needed since the `^` has a special meaning in a formula; wrapping as we do allows the standard usage in R, which is to raise `X` to the power `2`. We now perform a regression of `medv` onto `lstat` and `lstat^2`.

```{r}
lm.fit.quadr <- lm(medv ~ lstat + I(lstat^2), data = Boston)
summary(lm.fit.quadr)
```


The near-zero p-value associated with the quadratic term suggests that it leads to an improved model. We use the `anova()` function to further quantify the extent to which the quadratic fit is superior to the linear fit.


```{r}
lm.fit

lm.fit.quadr
```


```{r}
anova(lm.fit, lm.fit.quadr)
```


Here Model 1 represents the linear submodel containing only one predictor, `lstat`, while Model 2 corresponds to the larger quadratic model that has two predictors, `lstat` and `lstat^2`. The `anova()` function performs a hypothesis test comparing the two models. The null hypothesis is that the two models fit the data equally well, and the alternative hypothesis is that the full
model is superior. Here the F-statistic is 135 and the associated p-value is virtually zero. This provides very clear evidence that the model containing the predictors `lstat` and `lstat^2` is far superior to the model that only contains the predictor `lstat`. This is not surprising, since earlier we saw evidence for non-linearity in the relationship between `medv` and `lstat`. If we
output the diagnostic plots:

```{r}
autoplot(lm.fit, which = 1)
autoplot(lm.fit.quadr, which = 1)
```



In order to create a cubic fit, we can include a predictor of the form `I(X^3)`. However, this approach can start to get cumbersome for higherorder polynomials. A better approach involves using the `poly()` function to create the polynomial within `lm()`. For example, the following command
produces a fifth-order polynomial fit:

```{r}
lm(medv ~ poly(lstat, 5, raw = T), data = Boston) %>%
  summary()

# Note: we've added the `raw = T` argument to produce same coefficients as if the polynomial predictors were added manually
```



This suggests that including additional polynomial terms, up to fifth order, leads to an improvement in the model fit! However, further investigation of the data reveals that no polynomial terms beyond fifth order have significant p-values in a regression fit.
Of course, we are in no way restricted to using polynomial transformations of the predictors. Here we try a log transformation. 

```{r}
lm(medv ~ log(rm), data = Boston) %>%
  summary
```


# Qualitative Predictors

We will now examine the `Carseats` data, which is part of the `ISLR` package.
We will attempt to predict `Sales` (child car seat sales) in 400 locations
based on a number of predictors.

```{r}
data(Carseats)
names(Carseats)
```


The `Carseats` data includes qualitative predictors such as `Shelveloc`, an indicator
of the quality of the shelving location—that is, the space within a store in which the car seat is displayed—at each location. The predictor `Shelveloc` takes on three possible values, `Bad`, `Medium`, and `Good`.


Given a qualitative variable such as `Shelveloc`, R generates dummy variables automatically. Below we fit a multiple regression model that includes some interaction terms

```{r}
carseats.fit <- 
  lm(Sales ~ . + Income:Advertising + Price:Age, data = Carseats)

summary(carseats.fit)
```

The `contrasts()` function returns the coding that R uses for the dummy variables.

```{r}
contrasts(Carseats$ShelveLoc)
```

Use `?contrasts` to learn about other contrasts, and how to set them. 

R has created a `ShelveLocGood` dummy variable that takes on a value of `1` if the shelving location is `Good`, and `0` otherwise. It has also created a `ShelveLocMedium` dummy variable that equals `1` if the shelving location is `Medium`, and `0` otherwise. A `Bad` shelving location corresponds to a zero for each of the two dummy variables. The fact that the coefficient for
`ShelveLocGood` in the regression output is positive indicates that a good shelving location is associated with high sales (relative to a bad location).

And `ShelveLocMedium` has a smaller positive coefficient, indicating that a medium shelving location leads to higher sales than a bad shelving location but lower sales than a good shelving location.

