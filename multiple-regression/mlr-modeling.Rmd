---
title: "Множественная регрессия: построение и интепретация моделей"
author: "Заходякин Г.В., postlogist@gmail.com"
date: "14.02.2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
options(width = 100) # ширина текстового вывода
options(digits = 3) # число знаков после запятой в выводе 
```


# Модель множественной регрессии

Модель **множественной линейной регрессии** (*multiple linear regression*) позволяет предсказывать непрерывную выходную переменную на основе **нескольких** объясняющих переменных. При этом объясняющие переменные могут быть непрерывными или бинарными (0-1).

![Множественная регрессия - предсказание непрерывной выходной переменной на основе нескольких объясняющих переменных](figures/regression.png)

Модель множественной регрессии включает несколько объясняющих переменных, для каждой из которой используется отдельный коэффициент:

$$ y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_k x_k + \varepsilon $$



В этом уравнении $y$ - выходная переменная, а $x_1, x_2, \ldots, x_k$ - объясняющие переменные.

Угловые коэффициенты (*slope coefficients*) $\beta_1, \beta_2, \ldots, \beta_k$ измеряют влияние соответствующей объясняющей переменной с учетом влияния всех остальных объясняющих переменных (т.е. при прочих равных). Например, коэффициент $\beta_1$ показывает ожидаемое изменение $y$ при единичном увеличении $x_1$, в то время как остальные предикторы - $x_2, x_3, \ldots, x_k$ - будут зафиксированы. 

Таким образом, коэффициент для переменной в уравнении множественной регрессии отражает **предельный эффект** (*marginal effect*) этой переменной при зафиксированных значениях остальных переменных:


$$ \beta_j = \frac {\partial{y}} {\partial{x_j}} $$


**Свободный член** (*intercept*) $\beta_0$  отражает значение выходной переменной $y$ в точке, где все предикторы равны нулю.

Как и случае с простой линейной регрессией, для остатка модели $\varepsilon$ предполагаются следующие свойства:

1. **Среднее значение остатка - нулевое** (нет систематической ошибки) 

$$E(\varepsilon)=0$$

2. **Остатки модели некоррелированы между собой**:

$$\rho(\varepsilon_i, \varepsilon_k) = 0, \forall i \ne k$$

Если данное предположение нарушается, то модель будет неэффективной, т.е. можно построить более точную модель на тех же данных. Причина в том, что корреляция остатков между собой - это полезная информация, которая может быть учтена в модели для снижения ошибки. 


3. **Остатки модели некоррелированы с объясняющей переменной**:

$$\rho(\varepsilon, x_j) = 0, \forall x_j$$

Это предположение распространяется и на другие переменные в наборе данных. Если есть зависимость остатков от другой переменной, то можно улучшить точность модели, включив в нее эту переменную. 


4. **Дисперсия (и стандартное отклонение) остатков постоянна**, т.е. не зависит от $x$:

$$\sigma_\varepsilon = const, \forall x$$

Данное допущение называется **гомоскедастичностью** (homoscedasticity).

5. **Распределение остатков модели - нормальное**: $$\varepsilon \sim N(0, \sigma).$$


Оценка коэффициентов уравнения множественной регрессии: 


$$ y = b_0 + b_1 x_1 + b_2 x_2 + \ldots + b_k x_k + e $$

- выполняется методом наименьших квадратов, т.е. по критерию минимизации суммы квадратов остатков:


$$ \sum_{i=1}^N e_i^2 = \sum_{i=1}^N(y_i - b_0 - b_1 x_{i,1} - \ldots - b_k x_{i,k})^2. $$

В методе МНК используется формула в явном виде для вычисления коэффициентов. Поскольку эта формула предполагает громоздкие операции с матрицами, коэффициенты на практике всегда вычисляются с помощью компьютерных программ.

Для оценки модели (*model fitting*) в R используется та же функция `lm()`, что и для простой линейной регрессии.


# Пример разработки моделей множественной регрессии

В этом примере мы исследуем влияние различных факторов на расход топлива для автомобилей. Данные находятся в файле `datasets/cars.csv`.

## Подготовка

Загрузка пакетов

```{r Загрузка пакетов, message=FALSE}
library(readr) # считывание данных из текстовых файлов
library(tidyverse) # манипулирование данными 
library(ggplot2) # визуализация 
library(ggfortify) # визуализация диагностических графиков
library(modelr) # вспомогательные функции для работы с моделями
library(broom) # преобразование результатов моделирования в табличный вид
library(GGally) # построение матрицы диаграмм рассеяния 
library(car) # функции для степенных преобразований
library(forcats) # работа с факторами
```


Загрузка данных

```{r Загрузка данных, warning = FALSE, message = FALSE}
cars <- read_csv2("datasets/cars.csv", skip = 25) #в файле используются "русские" разделители, больше подходит read_csv2()
glimpse(cars)
```


## Разведочный анализ

### Пропущенные наблюдения

```{r Количество пропусков по столбцам}

cars %>%
  gather(key = 'Переменная', value = 'Значение') %>%
  group_by(`Переменная`) %>%
  summarise(`Пропуски` = sum(is.na(`Значение`))) %>%
  arrange(-`Пропуски`)

```

Количество строк с пропусками можно определить с помощью функции `complete.cases()`, которая выдает для каждой строки набора данных `TRUE` или `FALSE`, в зависимости от того, известны ли все значения переменных для примера в этой строке.

```{r В каких строках есть пропуски?}

cars %>% 
  filter(!complete.cases(.)) # . обозначает место, в которое надо подставить результат предыдущей операции

```

Всего в наборе данных содержится 13 пропущенных значений, строк с хотя бы одним пропуском - 5. Это небольшой процент от общего числа наблюдений (157), поэтому для дальнейшего анализа исключим наблюдения с пропущенными значениями.

```{r Удаление строк с пропусками}
cars2 <- na.omit(cars)

# Осталось наблюдений:
cat("Осталось", nrow(cars2), "наблюдений из", nrow(cars))
```

### Связи между переменными


Первые 5 столбцов в наборе данных содержат текстовые переменные, которые нельзя использовать для корреляционного анализа. Отберем только числовые столбцы и исследуем корреляции между ними.

```{r Только числовые переменные}
cars_num <- cars2 %>% select_if(is.numeric)

# Коэффициенты корреляции и из значимость
Hmisc::rcorr(as.matrix(cars_num))

# Матрица диаграмм рассеяния
ggpairs(cars_num, 
        lower = list(continuous = wrap("smooth_lm", color = 'blue')))
```

На диаграммах рассеяния видна зависимость между всеми переменными и `mpg`. Коэффициенты корреляции отрицательны и статистически значимы.

Для большого количества переменных матрица диаграмм рассеяния получается слишком громоздкой. Лучше исследовать связь каждой переменной с целевой в отдельности. Для этого придется преобразовать данные в "длинный" формат.

```{r Перевод в длинный формат}

cars_tall <- cars2 %>% 
  gather(key = 'Показатель', value = 'Значение', price:fuel_cap) # текстовые переменные и mpg надо оставить без изменений

```



```{r Диаграммы рассеяния для отдельных пар переменных}

ggplot(data = cars_tall,
       mapping = aes(x = `Значение`, y = mpg)) +
  geom_point(aes(colour = type), alpha = 0.5) +
  stat_smooth(method = lm, se = F) +
  facet_wrap(~ `Показатель`, scale = 'free_x', ncol = 2)

```

На этой диаграмме зависимости можно рассмотреть более подробно. Видно, что некоторые связи - нелинейные, есть выбросы, непостоянство дисперсии. Также заметно расслоение облака точек по типу машины - грузовик или легковой автомобиль. Эти особенности необходимо учесть при построении моделей.

## Моделирование

### Линейная регрессия

На основе результатов корреляционного анализа можно предположить, что наилучшим предиктором для топливной эффективности будет вес машины `curb_wgt`, т.к. коэффициент корреляции для этой переменной наибольший по модулю:

```{r Наибольшая корреляция}

cars_tall %>% 
  group_by(`Показатель`) %>%
  summarise(r = cor(mpg, `Значение`)) %>%
  arrange(desc(abs(r)))

```

Построим модель простой линейной регрессии по переменной `curb_wgt`:

```{r Простая линейная регрессия с curb_wgt}
m_wgt <- lm(mpg ~ curb_wgt, data = cars2)
summary(m_wgt)
```

В табличном виде коэффициенты модели можно получить с помощью функции `broom::tidy()`.

```{r Коэффициенты первой модели в виде таблицы}
tidy(m_wgt) %>% select(term:std.error) # эта же информация есть и в summary()
```


Получена модель:

$$ \widehat{mpg} = 42.556 - 5.543 \cdot curb\_wgt $$

На каждую тысячу фунтов пробег автомашины на 1 галлоне топлива сокращается на 5.5 миль.

По F-критерию модель значима. Также t-критерии показывают значимость коэффициентов модели.

Визуализация модели:

```{r Визуализация модели с curb_wgt}
ggplot(add_predictions(cars2, m_wgt, var = 'fit')) +
  geom_point(aes(curb_wgt, mpg)) +
  geom_line(aes(curb_wgt, fit), colour = 'red') + 
  labs(title = 'Прогноз mpg на основе curb_wgt')
```

Рассмотрим подробнее сводку по модели:

```{r Сводка по модели с curb_wgt}
glance(m_wgt) %>% select(r.squared, sigma) # эта же информация есть и в summary()
```

Модель объяснила 67.2% дисперсии исходных данных, ее стандартная ошибка равна 2.47 мили на галлон топлива.

### Множественная регрессия с двумя предикторами

Точность модели можно улучшить, если добавить в модель другие предикторы. Попробуем найти наилучшего кандидата на роль второго предиктора, посчитав корреляцию остальных переменных с остатками первой модели.

```{r Какая из оставшихся переменных - лучший второй предиктор?}

cars2 %>% 
  add_residuals(m_wgt, var = "residual") %>% #добавили остатки
  select_if(is.numeric) %>% # убрали текстовые столбцы
  select(-curb_wgt, -mpg) %>% #убрали уже учтенные в модели переменные 
  gather('var', 'value', price:fuel_cap) %>% #длинный формат данных
  group_by(var) %>% # сгруппировали по названию переменной
  summarise(r = cor(value, residual)) %>% # посчитали коэффициент корреляции для каждой группы
  arrange(desc(abs(r)))

```

Наибольший по модулю коэффициент корреляции с остатками первой модели - у переменной `horsepow` - мощность двигателя. 

Во второй модели рассмотрим одновременное влияние веса машины и ее мощности.

```{r Модель с весом и мощностью}
m_wgtpow <- lm(mpg ~ curb_wgt + horsepow, data = cars2)
summary(m_wgtpow)
```


Модель в целом значима. Все угловые коэффициенты и свободный член также значимы.

Получена модель:

$$ \widehat{mpg} = 42.57 - 4.78 \cdot curb\_wgt - 0.014 \cdot horsepow $$


Коэффициенты модели можно интерпретировать так:

- среди автомобилей одинаковой мощности, каждая дополнительная тысяча фунтов веса уменьшает пробег на 4.78 мили на галлон;

- среди автомобилей одинакового веса, каждая лошадиная сила снижает пробег на 0.014 мили на галлон.

Коэффициент детерминации $R^2$ немного улучшился: 69.3% вместо 67.2%


Сделаем по второй модели интервальный прогноз пробега на 1 галлоне для машины весом 4 тысячи фунтов и мощностью 200 лошадиных сил. Для этого нужно создать новый набор данных, содержащий эти две переменные:

```{r Интервальный прогноз по модели с curb_wgt и horsepow}

newcar <- tibble(curb_wgt = 4, horsepow = 200)
predict(m_wgtpow, newcar, interval = 'prediction', level = 0.9 )

```

Согласно прогнозу, можно ожидать, что пробег такой машины с 90% надежностью составит от ~17 до ~25 миль на одном галлоне.

Поскольку в уравнение регрессии входит 3 переменных, модель невозможно визуализировать на плоской диаграмме. Мы могли бы зафиксировать один из предикторов и построить диаграмму с учетом второго, однако этот подход неудобно использовать для моделей с большим числом предикторов. Поэтому просто сравним прогнозы модели с фактом.


```{r Соответствие прогноза и факта для модели с весом и мощностью}

ggplot(add_predictions(cars, m_wgtpow, var = 'fit'), 
       aes(fit, mpg)) +
  geom_point() +
  geom_line(aes(fit, fit), color = 'red', linetype = 'dashed') +
  labs(title = 'Соответствие прогноза и факта для модели с curb_wgt и horsepow',
       x = 'Прогноз mpg',
       y = 'Факт mpg')
  
```


При идеальном соответствии прогноза и факта была бы получена прямая линия. В реальности же наблюдается отклонение фактических значений от предсказанных.

Исследуем остатки модели.

```{r Исследование остатков модели с весом и мощностью}

autoplot(m_wgtpow)

```

Наблюдается слабо выраженная нелинейность. Также имеются отклонения от нормальности в хвостах распределения остатков, есть подозрение на гетероскедастичность и наличие влиятельных наблюдений.

```{r Влиятельные наблюдения}
m_wgtpow_fit <- m_wgtpow %>% 
  fortify() %>% # сохранили остатки, предсказания и показатели влиятельности в набор
  merge(., cars2) # добавили исходные данные (merge удаляет дублирующиеся при слиянии столбцы)

head(m_wgtpow_fit)

ggplot(m_wgtpow_fit, aes(.fitted, mpg)) +
  geom_point(aes(size = .cooksd, colour = .hat)) +
  geom_line(aes(.fitted, .fitted), 
            colour = "red", linetype = "dashed") +
  scale_colour_gradient(low = "black", high = "red") +
  ggrepel::geom_label_repel(
    data = filter(m_wgtpow_fit, .cooksd > 4/nrow(cars2)),
    mapping = aes(label = model), alpha = 0.25) +
  labs(title = "Влиятельные наблюдения",
       x = "Прогноз mpg", y = "Факт mpg", 
       colour = "Леверидж", size = "Расстояние Кука")
  

```

Наиболее влиятельным наблюдением является модель Viper, у которой наибольшее значение левериджа, т.е. удаленности от прочих наблюдений в пространстве предикторов (мощность, вес) и большое расстояние Кука (коэффициенты модели сильно меняются, если удалить это наблюдение). Также влиятельным наблюдением является модель Metro, у которой большой остаток и расстояние Кука.



### Регрессия с категориальными предикторами

Рассмотрим влияние мощности двигателя на пробег. Поскольку в данных имеются два типа машин - автомобили и грузовики, можно предположить, что расход топлива по-разному зависит от мощности для этих типов машин.

```{r Связь между мощностью и пробегом по типам машин}
ggplot(cars2, aes(horsepow, mpg, colour = type)) +
  geom_point() +
  geom_smooth(method = lm, se = F) +
  labs(title = "Связь пробега и мощности для двух типов машин",
       colour = "Тип машины")
```

При расщеплении данных по типу машины видно, что при той же мощности двигателя пробег на галлоне топлива для грузовиков значительно меньше. Линия регрессии для грузовиков смещена вниз по сравнению с линией для легковых машин. 

Для того, чтобы учесть в модели различие в типах машин, можно включить в модель **фиктивную переменную** (*dummy variable*), принимающую значение 0 или 1 в зависимости от типа машины. Значение 0 соответствует базовой категории (выбирается произвольно, в нашем случае примем, что базовая категория (*reference category*) - легковой автомобиль, тип Automobile), а значение 1 - категории, которая сравнивается с базовой (Truck).

**Замечание:** здесь эта операция сделана в иллюстративных целях. R может автоматически преобразовывать категориальные предикторы в наборы фиктивных переменных (см. далее).


```{r Регрессия по мощности с фиктивной переменной для типа машины}
carsdummy <- cars %>%
  mutate(truck = ifelse(type == "Truck", 1, 0))

m_pow_dummy <- lm(mpg ~ horsepow + truck, data = carsdummy)

summary(m_pow_dummy)
```

Получена модель:

$$\widehat{mpg} = 33.8 - 0.046 \cdot horsepow - 5.5 \cdot truck$$

Таким образом, для грузовиков (`truck = 1`), значение переменной `mpg` на 5.5 миль/галлон меньше, чем для автомобилей (`truck = 0`). Т.е. для грузовиков линия регрессии смещается вниз на 5.5. Коэффициент для фиктивной переменной в этой модели показывает отличие категории грузовиков от базовой: свободный член $b_0$ для автомобилей = 33.8, для грузовиков - 28.3.

R может автоматически добавлять фиктивные переменные, если в модель включены категориальные переменные (факторы). При этом базовой категории будет соответствовать первый уровень фактора. 
```{r Регрессия с мощностью и факторной переменной}

cars2f <- cars2 %>%
  mutate(type = factor(type, levels = c('Automobile', 'Truck')))


m_pow_factor <- lm(mpg ~ horsepow + type, data = cars2f)

summary(m_pow_factor)
```

Результаты идентичны модели, в которой фиктивная переменная добавлена вручную.


### Модель с учетом взаимодействия предикторов

Рассмотрим связь пробега на 1 галлоне топлива с другой переменной - `curb_wgt`.

```{r Зависимость пробега от веса по типам машин}
ggplot(data = cars2, aes (curb_wgt, mpg, colour = type)) +
  geom_point() +
  geom_smooth(method = lm, se = F) +
  labs(title = "Связь пробега и веса для двух типов машин",
     colour = "Тип машины")

```

На этом графике видно, что регрессионные прямые для подгрупп не просто смещены, у них разный наклон. Таким образом, оба коэффициента - свободный член и угловой коэффициент для подгрупп отличаются. Отличия углового коэффициента для подгрупп обусловлено **взаимодействием** (*interaction*) факторов: угловой коэффициент для веса зависит от значения другой переменной (тип машины). 

При моделировании взаимодействие включается в уравнение как произведение двух взаимодействующих переменных. Однако в R есть специальный синтаксис для взаимодействия:`var1:var2`.

```{r Регрессия с весом машины и взаимодействием двух факторов}

m_wgt_interact <- lm(mpg ~ curb_wgt + type + curb_wgt:type, data = cars2f)

summary(m_wgt_interact)

```

Получено уравнение модели:

$$\widehat{mpg} = 44.4 - 6.0 curb\_wgt - 12  (type=Truck) +  2.8 curb\_wgt (type=Truck)$$
  
  
    

Сравним модели для двух подгрупп.

Для легковых автомобилей (`type = Automobile`): 

$$\widehat{mpg} = 44.4 - 6 curb\_wgt $$

Для грузовиков `type = Truck`: 

$$\widehat{mpg} = (44.4 - 12.0) -6.0 curb\_wgt + 2.8 curb\_wgt $$

или:

$$ \widehat{mpg} = 32.4 - 3.2 curb\_wgt$$

Таким образом, получилось, что и свободный член, и угловой коэффициент для подгрупп отличаются.

Заметим, что в R можно получить модель, включающую как предикторы по отдельности, так и во взаимодействии, используя синтаксис формулы: `var1 * var2`.

```{r Сокращенный синтаксис для взаимодействия}
lm(mpg ~ curb_wgt * type, data = cars2f) %>%
  summary()
```

Результат идентичен предыдущей модели.

### Категориальные предикторы с более чем двумя категориями

Если в модель включен фактор с более чем двумя уровнями, то R автоматически добавит в уравнение фиктивные переменные для каждой категории, кроме базовой. Таким образом, можно будет оценить, как каждая из категорий отличается от базовой.

Рассмотрим пример фактора с 3 уровнями - страна производителя машины. В качестве базовой категории выберем Японию (предполагая, что японские машины - самые экономичные).


```{r Добавление фактора и визуализация зависимостей}
cars2f <- cars2f %>%
  mutate(country = factor(country, levels=c('Japan', 'Europe', 'USA')))

ggplot(cars2f, aes(horsepow, mpg, colour = country)) +
  geom_point() +
  geom_smooth(method = lm, se = F) + 
  labs(title = "Связь пробега и мощности для машин из разных стран",
     colour = "Страна производителя")

```

Заметно небольшое отличие между эффективностью японских и американских машин, при этом наклон прямых отличается слабо. Гипотеза об большей экономичности японских машин не подтвердилась.
Для европейских машин отличие в наклоне более выражено, т.е. у этих машин не только больше эффективность, но и при увеличении мощности эффективность падает не так сильно, как у двух других групп.

Вначале построим модель без учета взаимодействия факторов.

```{r Регрессия для связи мощности со страной происхождения}

m_country <- lm(mpg ~ horsepow + country, data = cars2f)

summary(m_country)
```

Из коэффициентов модели следует, что для европейских машин пробег на галлоне, в среднем, на 1.2 мили больше, чем для японских, а для американских машин - в среднем, на 0.3 мили больше, чем для японских.


Поскольку мы видели, что наклон регрессионных прямых для подгрупп отличается, необходимо построить модель со взаимодействием факторов.

```{r Регрессия для связи мощности со страной происхождения, с учетом взаимодействия}

m_country_interact <- lm(mpg ~ horsepow * country, data = cars2f)

summary(m_country_interact)
```

Включение всех взаимодействий привело к тому, что часть коэффициентов получились незначимыми. При увеличении количества переменных в модели каждый из них оценивается по меньшему количеству независимых данных (уменьшается число степеней свободы), поэтому растет стандартная ошибка оценки коэффициента. 

Мы видели, что наклон линий отличается существенно только для европейских машин, поэтому попробуем построить модель со взаимодействием только для этой категории машин. 


```{r Модель со взаимодействием только для европейских машин}

# Подготовка данных: добавляем новый фактор - европейская машина, или нет.

cars2f <- cars2f %>%
  mutate(europe = fct_recode(country,
                             No = "Japan",
                             No = "USA",
                             Yes = "Europe"))
  

m_europe_interact <- lm(mpg ~ horsepow * europe, 
                                data = cars2f)

summary(m_europe_interact)
```

Коэффициент для страны получился незначимым, однако взаимодействие страны и мощности значимо.

Уравнение модели:

$$\widehat{mpg} = 33.4 - 0.05 horsepow - 3.3  (europe=Yes) + 0.02 horsepow (europe=Yes)$$

Для американских и японских машин (europe = No):

$$\widehat{mpg} = 33.4 - 0.05 horsepow $$

Для европейских машин (europe = Yes):

$$\widehat{mpg} = 30.1 - 0.03 horsepow $$

Сравним прогнозируемую эффективность машин с одинаковой мощностью - 250 л.с. из разных стран:


```{r Прогнозирование для модели с факторами}

# Новые данные
new_cars <- data.frame(horsepow = c(250, 250), 
                       europe = factor(c("No", "Yes"), levels = c("No", "Yes")))

# Прогноз
pred_mpg <- predict(m_europe_interact, new_cars,
                    interval = "prediction",
                    level = 0.9) %>% 
  as.data.frame() %>%
  bind_cols(new_cars, .)

pred_mpg

```

