---
title: "Building and comparing classification models"
author: "Gleb Zakhodyakin"
date: '23.09.2019'
output: 
  html_document: 
    toc: yes
    toc_float: yes
---

# Introduction

This notebook provides a tutorial on solving a classification task using logistic regression and decision tree algorithms. Here we build and compare a few models trying to predict the survival of Titanic passengers. 


# Preparation

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(digits = 3) # Setting the output precision to 3 decimals
```


```{r, warning=FALSE, message=FALSE}
library(tidyverse) # Data transformation and visualization
library(scales) # Enables percentage scale for ggplot graphs
library(rpart) # Decision Trees: the CART algorithm
library(C50) # Another implementation of decision trees in R: C5.0
library(party) # Another tree-based algorithm: conditional inference trees
library(rpart.plot) # Nice visualization of rpart Decision Trees
library(partykit) # Visualization of Conditional Inference and C5.0 Trees
library(mlr) # MAchine learning framework for R. Here we are using model accuracy metrics provided by the package.
library(ROCR) # Plotting ROC
library(RColorBrewer) # Good color scales
library(modelr) # Working with models in tidyverse

```

The data are in `titanic.csv` file.

```{r, message=FALSE}
d <- read_csv("titanic.csv")
glimpse(d)
```

The dataset contains information on  `r nrow(d)` passengers.

Variables:
 - `survived` - the target (survived or not)
 - `pclass` - passenger class (first, second or third)
 - `sex` - gender
 - `age` - age, years
 - `sibsp` - the number of accompanying relatives - brothers, sisters or spouses
 - `parch` - the number of accompanying children or parents
 - `fare` - ticket price paid by the passenger

The variables `sibsp`, `parch`, `age` and `fare` are quantitative, the rest are categorical. From the summary above we can see, that the categorical variables are stored now as a text.

# The exploratory data analysis

## Data Preparation

Let's consider  distributions of variables and the presence of missing values.

```{r}
summary(d)
```

Variables `age` and `fare` contain NAs. The fare is unknown just for one passenger, but age is not provided for more than 260 passengers, orabout 20% of observations. This is a significant proportion of data, so we must decide on the strategy for replacing NAs in the `age` variable for the modeling purpose.

We must also convert the text variables into factors. It is a good idea to set the factor levels manually, otherwise the alphabetical ordering of factor levels will be used.

Let's also convert the target variable into a factor:

```{r}
d <- d %>% 
  mutate(survived = as.factor(survived))
```

The levels will be in the alphabetical order - `died`, `survived`. The last level (`survived`) will be considered a 'positive' outcome.

## Factors influencing the survival probability

Let's explore, how the survival probability correlates with different factors

### Passenger class

```{r}
ggplot(d) +
  geom_bar(aes(x = pclass, fill = survived), position = "fill") +
  scale_y_continuous(labels = percent) +
  labs(title = "Survival Probability vs Passenger Class",
       x = "Passenger class", y = NULL, fill = NULL)
```

The survival probability is correlated with the passenger class. The 1st class passengers have mostly survived, the 3rd class passengers had just a 25% of survival.

### Gender

```{r}
ggplot(d) + 
  geom_bar(aes(x = sex, fill = survived), position = "fill") +
  scale_y_continuous(labels = percent) +
  labs(title = "Survival Probability vs Gender",
       x = "Gender", y = NULL, fill = NULL)
```

The proportion of survived males is significantly lower.

Let's bring more factors into consideration and see, if the survival probability is dependent both on gender and the passenger class.

```{r}
ggplot(d) + 
  geom_bar(aes(x = sex, fill = survived), position = "fill") +
  facet_wrap(~ pclass) + 
  scale_y_continuous(labels = percent) +
  labs(title = "Survival probability vs Gender and Class",
       x = "Gender", y = NULL, fill = NULL)
```

Here we can see the interaction between gender and passenger class: almost all women among 1st class passengers have survived. But in the 3rd class the proportion of survived women is much lower. Such an interaction between variables should be incorporated into a model.


### Age

Let's see, how the survival probability depends on age of the passenger. To simplify visualization, we'll split all passengers into a few age groups.

```{r, fig.height=5, fig.width=10}
ggplot(d) + 
  geom_bar(aes(x = cut(age, breaks = seq(0, 80, by = 5), right = TRUE),
               fill = survived), position = "fill") +
  scale_y_continuous(labels = percent) +
  labs(title = "Survival probability vs Age",
       x = "Age of a Passenger", y = NULL, fill = NULL)
```

Here we observe that the relationship is non-linear. Small children under 5 years and very old people aged 75 or more have larger survival proportion. Despite this, all passengers aged between 654 and 75 years have died. We can also see a separate group of passengers with an unknown age.

It's important to know, how reliable are such estimates of the survival rate. To answer this question, we can plot the size of each age group.


```{r}
ggplot(na.omit(d)) + 
  geom_histogram(aes(x = age, fill = survived), bins = 40) +
  labs(title = "The number of passengers in each age group",
       x = "Age", y = "Number of Passengers", fill = NULL)
```

Here we can see, that the number of elder passengers is quite small, so we won't care much about the peak of survival probability for the 75-80 years group.

Now, let's consider if the relationship between age and survival is the same for different passenger classes.


```{r, fig.height=7, fig.width=10}
ggplot(d) + 
  geom_bar(aes(x = cut(age, breaks = seq(0, 80, by = 5), right = TRUE),
               fill = survived), position = "fill") + 
  facet_wrap(~pclass, ncol = 1) +
  scale_y_continuous(labels = percent) +
  labs(title = "Survival Probability vs Age and Class",
       x = "Age of a Passenger", y = NULL, fill = NULL)  

```

We can see, that in the 1st class almost all children under 15 have survived, and most seniors above 50 have died. Again, this trend is different between the classes.
This should be accounted for while modeling. In a logistic regression model each predictor contributes linearly to the outcome. In case of age, the probability of survival should be proportional to age. To model the non-linear relationship between age and survival we've just discovered, we can **discretize** the predictor variable. This will allow a non-proportional contribution for different age groups.


```{r, fig.height=5, fig.width=10}
d <- d %>%
  mutate(agef = cut(age, 
                    breaks = c(0, 15, 50, 85),
                    labels = c("child", "adult", "senior"),
                    right = FALSE)) %>%
  mutate(agef = fct_explicit_na(agef, na_level = "unknown"))

ggplot(d) + 
  geom_bar(aes(x = agef,
               fill = survived), position = "fill") +
  facet_wrap(~pclass) +
  scale_y_continuous(labels = percent) +
  labs(title = "Probability of Survival vs Age and Class",
       x = "Age Group", y = NULL, fill = NULL)  

```

Since the relationship is different for different passenger classes, an interaction between factors should be incorporated into the model.

### Companions - siblings and spouses

Let's see, how the presence of companions is correlated to survival. Here we discretize the `sibsp` variable to ensure that the bars on the plot take only integer steps.

```{r}
ggplot(d) + 
  geom_bar(aes(x = factor(sibsp),
               fill = survived), position = "fill") +
  scale_y_continuous(labels = percent) +
  labs(title = "Survival Probability vs Siblings & Spouses",
       x = "Number of siblings and spouses", y = NULL, fill = NULL)  
  
```

The best chances for survival have the passengers with one or two companions. All members of very large groups have died.

Now, we consider, if this relationship depends on the class.


```{r}

ggplot(d) + 
  geom_bar(aes(x = factor(sibsp),
               fill = survived), position = "fill") + 
  facet_wrap(~pclass, ncol = 1) + 
  scale_y_continuous(labels = percent) +
  labs(title = "Survival probability vs Siblings, Spouses and Class",
       x = "Number of Siblings, Spouses Class", y = NULL, fill = NULL)  

```

The relationship differs slightly for different classes. Within the 3rd class the chances to survive were the best for passengers with one companion. The chances for lerger groups were better in the 1st and the 2nd classes.

To accomodate the non-linear relationship, we'll again discretize the number of companions into the following groups: single passengers, small groups (1-2 companions), large groups (3+ companions).


```{r, fig.height=5, fig.width=10}
d <- d %>%
  mutate(sibspf = cut(sibsp, 
                      breaks = c(0, 1, 3, 9),
                      labels = c("single", "small","large"),
                      right = FALSE))


ggplot(d) + 
  geom_bar(aes(x = sibspf,
               fill = survived), position = "fill") +
  facet_wrap(~pclass) +
  scale_y_continuous(labels = percent) +
  labs(title = "Survival Probabiilty vs Siblings, Spouses and Class",
       x = "Group Size", y = NULL, fill = NULL)  

```

### Companions - children and parents

We'll treat accompanying children and parents in a similar way.

Survival probability vs the number of companions:

```{r}
ggplot(d) + 
  geom_bar(aes(x = factor(parch),
               fill = survived), position = "fill") +
  scale_y_continuous(labels = percent) +
  labs(title = "Survival Probability vs Number of Parents and Children",
       x = "Number of Parents and Children", y = NULL, fill = NULL)  
```

By class:

```{r}
ggplot(d) + 
  geom_bar(aes(x = factor(parch),
               fill = survived), position = "fill") +
  facet_wrap(~pclass, ncol = 1) +
  scale_y_continuous(labels = percent) +
  labs(title = "Survival Probability vs Number of Parents, Children and Class",
       x = "Number of Parents and Children", y = NULL, fill = NULL)  
```

The relationship is similar to the one for siblings and spouses: single passengers and large groups have the least chances to survive. Travelling in a small group does increase the probability to survive.

Let's discretize groups using the same thresholds, as above:


```{r, fig.height=5, fig.width=10}

d <- d %>%
  mutate(parchf = cut(parch, 
                      breaks = c(0, 1, 3, 10),
                      labels = c("single", "small", "large"),
                      right = FALSE))


ggplot(d) + 
  geom_bar(aes(x = parchf,
               fill = survived), position = "fill") +
  facet_wrap(~pclass) +
  scale_y_continuous(labels = percent) +
  labs(title = "Survival Probability vs Number of Children, Parents and Class",
       x = "Number of Children and Parents", y = NULL, fill = NULL)  

```


### The Fare

Now let's see, if the fare paid is correlated to the survival probability

```{r}
ggplot(na.omit(d)) + 
  geom_histogram(aes(x = fare, fill = survived), position = "fill", bins = 20) +
  facet_wrap(~pclass, ncol = 1) +
  scale_y_continuous(labels = percent) +
  labs(title = "Survival Probability vs Fare",
       x = "Fare", y = NULL, fill = NULL)   

```

The relationship is prominent only for the 1st and 2nd class passengers. It seems that only for these classes the fare paid by the passenger determined the location of his/her cabin, and the survival probability as a result. In the 3rd class, the probability was equally low for everyone regardless the fare.

Let's see, if there's a missed influence from gender or age.

```{r}
ggplot(na.omit(d)) +
  geom_jitter(aes(age, fare, colour = survived)) +
  facet_grid( pclass ~ sex, scale = "free_y")
```

There's no difference in fares by gender. The age is correlated to the fare. For example, we can see, that for children below 15 the parents didn't buy the cheapest tickets. The cheapest fares were paid by adults in the 2nd and the 3rd class. This category faced the lowest survival probability. This fact might explain the increase in survival probability for higher fares.



# Modeling


## Train-test split

To estimate the predictive accuracy of our model, we'll test it on the new data. For this, we'll split the dataset into the training and the testing samples.

```{r}
set.seed(12) # Setting the random seed for reproducibility
train_ind <- sample(nrow(d), 900)
d_train <- d[train_ind, ] # Training sample
d_test <- d[-train_ind, ] # Test sample
```

## Logistic Regression with gender and class only

As a first step, we build a logistic regression model using the two most important predictors - the gender and the class of a passenger. We also include the interaction of these two factors, as we've seen the different effect of gender for different classes:

```{r}
m_lr_sexclass <- glm(survived ~ pclass + sex + pclass : sex, 
                     data = d_train,
                     family = binomial(link = "logit"))
summary(m_lr_sexclass)
```

### Interpreting the coefficients


A slope coefficient in a logistic regression model shows the change in logit, i.e. the log-odds for unit change of the predictor. Since $\ln a - \ln b = \ln a/b$, we can treat the slope coefficient as log-odds ratio for unit change in a predictor. Zero slope corresponds to no change.  

Instead of log odds ratio, we'd better compare the odds ratio itself. So we can exponentiate the slope coefficients:

```{r}
coefficients(m_lr_sexclass) %>% exp()
```

The exponentiated slope coefficients provide the odds-ratios for unit change in the predictor. Odds-ratio equal to 1 means no change.

In our model all predictors are dummy variables, so the coefficients show the difference for the corresponding groups of passengers.

Our baseline category is the 1st class female passenger. For such passenger, the model would predict survival odds of $\exp(3.517) = 33.7$. This corresponds to survival probability of $33.7/(33.7 + 1) = 0.971$.

A 2nd class female passenger has less chances to survive, as we can see from the negative sign of the slope coefficient for  `pclass2nd` dummy variable. The odds-ratio is $\exp(-1.539) = 0.215$. This means, that survival odds for a 2nd class female passenger would be about 5 times lower compared to the 1st class: $33.7 * 0.215 = 7.25$. This corresponds to survival probability of $ 7.25 / (7.25 + 1) = 0.878 $.

Similarly, from the negative slope coefficient for the `sexmale` variable we can conclude, that males have lower odds to survive. The odds-ratio is $\exp(-4.433) = 0.0119$. So for a 1st class male passenger survival odds would be $33.7 * 0.0119 = 0.4$, and survival probability $0.4 / (1+0.4) = 0.286$.


Since we've included the interaction terms, the survival odds for a 2nd or 3rd class male passengers can be computed by plugging in the numbers into the model we've built.

Let's calculate predictions for a few cases using our model:

 - Survival odds for a man travelling 2nd class: $O = \exp(3.517 - 4.433 - 1.539 + 0.549) = 0.149$   
 Survival probability: $P = 0.149 / 1.149 = 0.13$

- Survival odds for a man travelling 3rd class: $O = \exp(3.517 - 4.433 - 3.582 + 2.786 ) = 0.181$  
Survival probability: $P = 0.181 / 1.181 = 0.153$


We can also do inference using our model. Usually the training data is considered a sample, so we could do interval estimation of the population's coefficients:

```{r}
confint(m_lr_sexclass) %>% exp()
```



### Alternative method of model interpretation

Instead of treating the slope coefficients one by one, we can also just create predictions for a few test cases and compare the predicted survival probabilities.


```{r}
newdata  <-  
  tribble(~sex,     ~pclass,
          'female', '1st',
          'female', '2nd',
          'female', '3rd',
          'male',   '1st',
          'male',   '2nd',
          'male',   '3rd')


newdata %>% 
  add_predictions(m_lr_sexclass, type = 'response', 
                  var = 'survival_probability')

```


**Note:**  since our model uses factors, it's important to make sure that the factor levels have the same values and order both in training and new data. In the above example it is achieved by enumerating all the possible levels for each factor. To be sure, we could also manually specify the factor levels for the new data.


## Logistic regression for all available predictors

Now we'll build a more complex model that includes all of the relationships and interactions we've discovered at the exploratory data analysis stage. We're adding the age group and the number of companions, as well as the interaction with the pclass variable for all predictors.

```{r}
m_lr_all <- glm(survived ~ (sex + agef + sibspf + parchf + sex) * pclass, 
                data = d_train,
                family = binomial(link = "logit"))
summary(m_lr_all)
```

The model has become too complex. Due to large number of predictors, the standard errors for the slope coefficients were inflated and the coefficients became non-significant. 


## Logistic regression - a simplified model

The stepwise method can also be used for logistic regression models. Here we apply it to simplify the model using AIC:

```{r}
m_lr_step <- step(m_lr_all)
```

The final model is:

```{r}
summary(m_lr_step)
```

The simplified model includes main effects for all factors (gender, age group, class, number of siblings/spouses) except the number of parents and children.
The interaction effects are included for gender, class and age group.



## Decision tree - rpart

The classification task can be solved also by decision trees. In R, the `rpart` package implements the CART  algorithm for building decision trees.

In contrast to logistic regression, decision tree is a **local approach**. A tree splits the predictor space into separate partitions, where in each partition the relationships between predictors and the target may be completeley different compared to other partitions. Because of this, the non-linear relationships in data are automatically approximated by the tree, and we do not care much about modeling these non-linearities, as well as interactions. For example, we could see in the EDA phase, that the survival probability was higher for young children and seniors. To model this correctly with a logistic regression, we've discretized the data. When using decision trees, this is no longer necessary.

Decision tree algorithms can also handle missing data better. If a value of a quantitative variable is missing, a tree-based algorithm can do a **surrogate split** using other predictors. For missing categorical data, NA's are treated a separate category. However, it's recommended to fill the missing values to improve the quality of the model.

```{r}
m_rpart <- rpart(survived ~ pclass + sex + sibsp + parch + age,
                 data = d_train)
rpart.plot(m_rpart)

```


The first split is by gender, which is the most important predictor. The tree predicts death for all men older than 9 years. Younger men would survive when accompanied by a large number of siblings. 

For women, the next split is done by passenger class. For 1st and 2nd class female passengers, the tree predicts survival. For women in the 3rd class, the large number of accompanying siblings and spouses makes survival more probable.

Interestingly, the threshold for the number of companions is the same as we've used for the logistic regerssion. But age has a different threshold - 9 years instead of 15. 

The purity of the leaf nodes is quite low. For example, the first leaf (counting from the left hand side) is associated with `died` class label. But only about 84% of the passengers in this leaf have actually died.

The `rpart` implementation of the CART algorithm uses different approaches to prevent overfitting and to simplify the trees it produces. In this case, the result is perhaps simplified too much.

We can control the algorithm parameters associated with the tree complexity, such as minimum split, maximum depth, or complexity parameter. Refer to the help to learn, how to set them: `?rpart` and `?rpart.control`.



## Decision tree - ctree

Let's try a different implementation of the decision tree algorithm - the Conditional Inference Tree provided by the `party` package.

```{r, fig.height=7, fig.width=12}
m_ctree <- ctree(survived ~ factor(pclass) + factor(sex) + age + sibsp + parch,
                 data = d_train)
plot(m_ctree)
```

The tree produced by `ctree` is similar to the previous one. It is also small and has relatively impure leaves.

## Decision tree - C5.0


Now we try another decision tree algorithm - C5.0. The graphical output doesn't look nice when categorical variables are used, so we use the text-based representation here:

```{r}
m_c50 <- C5.0(survived ~ pclass + sex + age + sibsp + parch,
                 data = d_train)

summary(m_c50)
```

The tree is also very similar to `rpart', but it uses different predictors for classifying the male passengers.


Finally, we'll build one more model using the C5.0 algorithm. This is an ensemble mode consisting of decision trees.

```{r}
m_c50_boost <- C5.0(survived ~ pclass + sex + age + sibsp + parch,
                 data = d_train, trials = 20)
summary(m_c50_boost)
```


# Comparing the classifiers

## Classifier performance metrics

The performance of classifiers is assessed by comparing the predicted and the actual class labels on the test sample. 

Let's consider a few classifier performance metrics on the example of logistic regression. We'll use the simplest 2-predictor model we've constructed in the first step.

### Interpreting the confusion matrix and the performance metrics

The logistic regression model predicts not the class label, but the probability of the 'positive' outcome. To get the class label, we must introduce an arbitrary threshold. The 'positive' class label will be assigned whenever the predicted probability of the positive outcome is above this threshold. We'll use a 50% threshold in this example.

```{r}
# Predict the survival probability for the test data
pr_lr_sexclass <- predict(m_lr_sexclass, newdata = d_test, type = "response")

# Predict the class label
cat("Threshold 0.5:\n")
cl_lr_sexclass <- ifelse(pr_lr_sexclass > 0.5, "survived", "died")

# The confusion matrix
table(predicted = cl_lr_sexclass, actual = d_test$survived)

```

The confusion matrix contains the following elements:

 - $TN = 224$ - true negative cases (death is predicted, and the passenger has died)
 
 - $TP = 106$ - true positive cases (survival is predicted, and the passenger has survived)
 
 - $FP = 39$ - false positive cases (survival is predicted, but the passenger has died)
 
 - $FN = 40$ - false negative cases (death is predicted, but the passenger has survived)
 
 
The test sample has 409 passengers:

```{r}
nrow(d_test)
```

Using the confusion matrix, we can compute the model [**performance metrics**](https://en.wikipedia.org/wiki/Sensitivity_and_specificity):

**Accuracy** (Overall Success Rate) - a proportion of correctly classified cases:


$$ ACC = \frac{TP  + TN}{TP + TN + FP + FN} = \frac{245+67}{409} = 76.3\% $$

The accuracy shows, how often the model guessess the correct class label.


**Mean misclassification error** (overall error rate) - a proportion of cases classified incorrectly:


$$ MMCE = \frac{FP  + FN}{TP + TN + FP + FN} = \frac{5 + 92}{409} = 23.7\% $$

The mean misclassification error shows, how often the model assigns an incorrect class label.

**Sensitivity** (true positive rate, recall) - a proportion of positive examples in the dataset that were correctly labelled as positive:

$$ TPR =  \frac{TP}{P} = \frac{TP}{TP + FN} = \frac{67}{67 + 92} = 42.1\% $$

The sensitivity shows how sensitive is the model while detecting the positive cases. A less sensitive model will allow to detect only a small proportion of the positive cases present in the data. A very sensitive model would detect most of positive cases.


**Specificity** (true negative rate) - a proportion of the negative examples, that were correctly labelled by the model as negative:

$$ TNR =  \frac{TN}{N} = \frac{TN}{TN + FP} = \frac{245}{245 + 5} = 98.0\% $$

Specificity is similar to sensitivity, but here we are interested more in negative cases.


**False positive rate** (fall-out) - a proportion of incorrectly labelled negative examples:

$$ FPR =  \frac{FP}{N} = \frac{FP}{TN + FP} = \frac{5}{245 + 5} = 2.0\% $$

The False Positive Rate shows the proportion of negative cases, that were mislabelled as positive. Consider a model detecting spam e-mail. The 'false positive' would be the case, when such model would misclassify a 'good' e-mail as spam. The false positive rate in this case would be the proportion of good e-mails ended-up in the 'spam' folder.

We can see, that $FPR = 1 - TNR$, or  $1 - Specificity$.


**Precision** (positive predictive value) - the reliability of classification for the positive cases. This is a proportion of the cases labelled as 'positive', that are positive indeed.

$$ PPV =  \frac{TP}{TP + FP} = \frac{67}{67 + 5} = 93.1\% $$

This shows, that we can be pretty sure, that when a model predicts survival for a passenger, she would survive indeed.
In our example of spam-classification model, the precision would show the proportion of true spam letters in the 'spam' folder.

To simplify the performance assessment, we'll create a function that will compute the above performance metrics. The arguments of such function will be two vectors containing the predicted and the actual class labels, class label names, and an optional vector of probabilities for the positive outcome. The metrics are computed using the functions available in the `mlr` package.


```{r}

# The function for computing classifier performance metrics
perf <- function(predicted, actual, positive, negative,
                 probabilities = NULL ) {
  p <- 
    c(
      Accuracy = measureACC(actual, predicted),
      kappa = measureKAPPA(actual, predicted),
      TPR = measureTPR(actual, predicted, positive = positive),
      TNR = measureTNR(actual, predicted, negative = negative),
      FPR = measureFPR(actual, predicted, negative = negative, positive = positive),      
      PPV = measurePPV(actual, predicted, positive = positive)
  )
  
  if (!is.null(probabilities)) {
    p <- c(p, AUC = measureAUC(probabilities = probabilities, 
                               truth = actual, 
                               negative = negative,
                               positive = positive))
  }
  p
}

```


Let's test the function on the same logistic regression model with two predictors:


```{r}
# Prediction of class labels for passengers
cat("Threshold 0.5:\n")
cl_lr_sexclass <- ifelse(pr_lr_sexclass > 0.5, "survived", "died")

# Confusion matrix
table(predicted = cl_lr_sexclass, actual = d_test$survived)
perf(cl_lr_sexclass, d_test$survived, positive = "survived", negative = "died") * 100
```

The metrics have the same values, as we've calculated manually. The kappa metric will be discussed below.

### How the classification threshold influences the metrics

We've arbitrarily selected the threshold of 50% for survival probability to predict survivals. The classification threshold does influence the results a lot: 

- The lower the threshold, the more predicted positive outcomes there will be, this will increase the TPR. We increase the model's sensitiviy this way. However this will also raise the number of false positives, so FPR will also increase.

- When the threshold is raised, we need more confidence to classify a case as positive, so the FPR will decrease. But, at the same time, the TPR will also decrease. The sensitivity of the model will be lower.

Let's compare the performance metrics for different classification thresholds:


```{r}


cat("Threshold 0.30:\n")
cl_lr_sexclass_30 <- ifelse(pr_lr_sexclass > 0.3, "survived", "died")
table(predicted = cl_lr_sexclass_30, actual = d_test$survived)
perf(cl_lr_sexclass_30, d_test$survived, positive = "survived", negative = "died") * 100


cat("\nThreshold 0.9:\n")
cl_lr_sexclass_90 <- ifelse(pr_lr_sexclass > 0.9, "survived", "died")
table(predicted = cl_lr_sexclass_90, actual = d_test$survived)
perf(cl_lr_sexclass_90, d_test$survived, positive = "survived", negative = "died") * 100
```


### ROC-analysis

The trade-off between sensitivity (TPR) and the FPR of a model can be explored using the Receiver Operating Curve (ROC).

```{r}

performance_lr_sexclass <-  
  prediction(pr_lr_sexclass, d_test$survived) %>% 
  ROCR::performance("tpr", "fpr")

plot(performance_lr_sexclass, colorize = TRUE,
     main = "ROC for logistic regression with two predictors - gender and class")

```

From the plot we can see, that when the classification threshold (the color) is increasing, the FPR goes down, but the sensitivity (TPR) does this as well.

When threshold = 1, there will be no false positives (FPR = 0), but the model won't find any positive cases at all (TPR = 0). When threshold is 0, all cases will be classified as positive, so TPR = 1. But all negative examples will be classified incorrectly, so FPR is also 1.

The value of the model arises from the fact, that when we start to lower the threshold, the sensitivity grows faster, than the FPR. At some point, the rates of change in these two metrics will change, and the increase in FPR will be larger than the increase in sensitivity.

We can find a 'compromise' for the threshold, where a large proportion of positive cases is detected, but the FPR is still low.


The ROC curve for a classifier selecting a class label randomly would look like a 45-degree line, connecting (0, 0) and (1, 1) points. Changing the threshold for such a classifier would change TPR and FPR by the same amount, so there's no gain in performance.

An ideal classifier would detect all positive cases while making no false positive error. For such a classifier, the ROC curve would look like a L-shaped polyline, that has TPR = 1 right at the zero FPR. This can be rarely observed in practice, but we can compare the ROC for a particular classifier to this 'ideal' shape to judge upon its performance.

To quantify the performance of a classifier, the Area Under Curve (AUC) metric is used. One cas see, that the ideal classifier would have an AUC = 1. The worthless classifier would have AUC exactly half of this: AUC = 0.5. The actual classifiers would have $0.5 < AUC < 1. When AUC < 0.5, the classifier is worse than the random classifier. Probably, this is due to mixed up class labels.

The AUC metric can be computed using the `mlr::measureAUC()` function. The predicted probabilities for a positive outcome and the true class labels have to be provided as arguments. 

```{r}
measureAUC(probabilities = pr_lr_sexclass, truth = d_test$survived, positive = "survived") * 100
```


This metric will be computed by the `perf()` function we've defined above, if a vector of probabilities is provided to it.

```{r}
perf(cl_lr_sexclass_90, d_test$survived, positive = "survived", negative = "died",
     probabilities = pr_lr_sexclass) * 100
```


When interpreting the results, one can refer to the following ranges:

- A worthless classifier $AUC < 0.6$
- A weak classifier $0.6 \le AUC < 0.7$
- An acceptable classifier $0.7 \le AUC < 0.8$
- A good classifier $0.8 \le AUC < 0.9$
- An excellent classifier $0.9 \le AUC < 1.0$

Note, that the ranges are selected subjectively, and the expected performance is highly dependent on a particular type of classification problem.

Also note, that two classifiers with the same AUC might behave differently at different levels of FPR. So, in addition to looking at AUC, we usually compare the shapes of ROC on the plot.

### Cohen's kappa

Using the Accuracy metric can be misleading, when the classes are not balanced, i.e. when one of the class labels is very rare. For example, a proportion of fraudulent transactions is usually very low, for example 1 transaction in a million is fraudulent. A model, that treats all transactions as 'good' would have a very high accuracy under this setting (close to ACC=100%).  But despite the high accuracy it will be completely useless for its purpose of finding the fraudulent transactions.

To take into account the probability of just 'guessing correctly', the Kohen's kappa metric is used. This metric compares the model's performance to the performance of a random classifier, used on the same data. The probabilities for labels, assigned by such a random classifier would be equal to the proportions of the labels in the training data.

Cohen's kappa is calculated as:

$$ \kappa = \frac {ACC - ACC_e} {1 - ACC_e} $$,

where $ACC_e$ - is an expected accuracy of a random classifier, that is computed based on the marginal probabilities for each class label (either predicted or actual):

$$ ACC_e = P(predicted +) \cdot  P(actual +) + P(predicted -) \cdot  P(actual -) $$

Let's calculate $\kappa$ for our model. To simplify calculation, we can output the confusion matrix with proportions instead of counts using the `prop.table()` function. Then, the probability of the positive prediction is the sum for the last row of the table, i.e. TP + FP. 

```{r}
table(predicted = cl_lr_sexclass, actual = d_test$survived) %>% prop.table()
```

The expected accuracy of a random classifier:

```{r}
(0.0122 + 0.1638) * (0.2249 + 0.1638) + (0.5990 + 0.2249) * (0.5990 + 0.0122)
```

The accuracy of our model:

```{r}
0.5990 + 0.1638
```


Cohen's $\kappa$:

```{r}
(0.763 - 0.572) / (1 - 0.572)
```

In R Cohen's $\kappa$ can be computed using the `mlr::measureKAPPA()` functions:

```{r}
measureKAPPA(truth = d_test$survived, response = cl_lr_sexclass)
```

When interpreting $\kappa$ the following ranges can be used (Landis JR, Koch GG The measurement of obsever agreement for categorical data. Biometrics. 1997; 33:159-174):

- unacceptable agreement of the classifier $\kappa < 0.2$
- acceptable agreement $0.2 \le \kappa < 0.4$
- a fair agreement $0.4 \le \kappa < 0.6$
- a good agreement $0.6 \le \kappa < 0.8$
- an excellent agreement $0.8 \le \kappa < 1.0$


Cohen's $\kappa$ is also calculated using the `perf()` function we've defined above.

## Performance of the classifiers for the Titanic dataset

### The two-predictor logistic regression, 50% threshold

```{r}
cat("The 2 predictor logistic regression, 50% threshold:\n")
# Матрица ошибок классификации (confusion matrix)
table(predicted = cl_lr_sexclass, actual = d_test$survived)
perf(cl_lr_sexclass, d_test$survived, 
     positive = "survived", negative = "died",
     probabilities = pr_lr_sexclass) * 100
```


### The logistic regression with all available predictors

```{r}
pr_lr_all <- predict(m_lr_all, newdata = d_test, type = "response")
cl_lr_all <- ifelse(pr_lr_all > 0.5, "survived", "died")


cat("The logistic regression with all available predictors, 50% threshold:\n")

table(predicted = cl_lr_all, actual = d_test$survived)
perf(cl_lr_all, d_test$survived, positive = "survived", negative = "died",
     probabilities = pr_lr_all) * 100
```



### The simplified logistic regression model from the stepwise procedure

```{r}
pr_lr_step <- predict(m_lr_step, newdata = d_test, type = "response")
cl_lr_step <- ifelse(pr_lr_step > 0.5, "survived", "died")

cat("The stepwise logistic regression, 50% threshold:\n")

table(predicted = cl_lr_step, actual = d_test$survived)
perf(cl_lr_step, d_test$survived, positive = "survived", negative = "died",
     probabilities = pr_lr_step) * 100

```


### Decision tree - rpart

```{r}
pr_rpart <- predict(m_rpart, newdata = d_test, type = "prob")[, "survived"]
cl_rpart <- predict(m_rpart, newdata = d_test, type = "class")

cat("The rpart decision tree:\n")

table(predicted = cl_rpart, actual = d_test$survived)
perf(cl_rpart, d_test$survived, positive = "survived", negative = "died",
     pr_rpart) * 100

```

### Decision tree - ctree

```{r}

#The ctree implementation can't handle NAs, so here we replace missings with the median values in the test dataset.
d_test_cleaned <- 
  d_test %>%
  mutate(age = coalesce(age, median(age, na.rm=T)))

pr_ctree <- predict(m_ctree, newdata = d_test_cleaned, type = "prob") %>% 
  .[, 'survived'] %>% 
  as.numeric()

cl_ctree <- predict(m_ctree, newdata = d_test_cleaned, type = "response")


cat("The ctree decision tree:\n")

table(predicted = cl_ctree, actual = d_test$survived)
perf(cl_ctree, d_test$survived, positive = "survived", negative = "died",
     probabilities = pr_ctree) * 100


```

### Decision tree - C5.0

```{r}
pr_c50 <- predict(m_c50, newdata = d_test, type = "prob")[, "survived"]
cl_c50 <- predict(m_c50, newdata = d_test, type = "class")


cat("The C5.0 decision tree:\n")

table(predicted = cl_c50, actual = d_test$survived)
perf(cl_c50, d_test$survived, positive = "survived", negative = "died",
     probabilities = pr_c50) * 100
```

### Decision tree - C5.0 with boosting

```{r}
pr_c50_boost <- predict(m_c50_boost, newdata = d_test, type = "prob")[, "survived"]
cl_c50_boost <- predict(m_c50_boost, newdata = d_test, type = "class")

cat("The C5.0 decision tree with boosting:\n")

table(predicted = cl_c50_boost, actual = d_test$survived)
perf(cl_c50_boost, d_test$survived, positive = "survived", negative = "died",
     probabilities = pr_c50_boost) * 100

```


## Comparing the ROCs

By looking at AUC we conclude that the best model is an ensemble of C5.0 decision trees (obtained by a boosting procedure). For this model AUC = 84.8.

The next best two models are the stepwise logistic regression and the `ctree` decision tree, both having AUC = 83.

Now we compare the ROC curves for all constructed models visually. In R, the `ROCR` package can be used for this. 

To plot a ROC curve, we take the following steps:

 1. Using the  `prediction()` function that takes a vector of predicted positive class probabilities and a vector of actual class labels we create an object that is later used for calculating performance.

 2. Using the  `ROCR::performance()` function, that takes an object producted in step 1, we compute the performance metrics for plotting the ROC. Since a function with the same name exists also in the `mlr` package, which we also are using here, we need to clarify, which function to use by adding a prefix with the package name: `ROCR::` to to its name.
 
 3. The object, produced in step 2, can be plotted using the `plot()` function. 


**Computing ROC for each model**

```{r}
perf_lr_sexclass <- prediction(pr_lr_sexclass, d_test$survived) %>% 
  ROCR::performance("tpr", "fpr")
perf_lr_all <- prediction(pr_lr_all, d_test$survived) %>% 
  ROCR::performance("tpr", "fpr")
perf_lr_step <- prediction(pr_lr_step, d_test$survived) %>% 
  ROCR::performance("tpr", "fpr")
perf_rpart <- prediction(pr_rpart, d_test$survived) %>% 
  ROCR::performance("tpr", "fpr")
perf_ctree <- prediction(pr_ctree, d_test$survived) %>% 
  ROCR::performance("tpr", "fpr")
perf_c50 <- prediction(pr_c50, d_test$survived) %>% 
  ROCR::performance("tpr", "fpr")
perf_c50_boost <- prediction(pr_c50_boost, d_test$survived) %>% 
  ROCR::performance("tpr", "fpr")

```

**Comparing ROCs**

The `ROCR` package uses base R graphics. If required, you can find examples of working with this plotting system [here](http://www.statmethods.net/graphs/index.html). The Flowing Data blog has a [post](http://flowingdata.com/2016/03/22/comparing-ggplot2-and-r-base-graphics/) comparing ggplot2 and base graphics plotting systems.

```{r}
# Generating a 7 color palette (we've built 7 models):
pal <- RColorBrewer::brewer.pal(7, "Set1") 

# Plotting the first ROC
plot(perf_lr_sexclass, col = pal[1], main = "Comparing the models")

# Adding the remaining ROCs
plot(perf_lr_all, col = pal[2], add = TRUE)
plot(perf_lr_step, col = pal[3], add = TRUE)
plot(perf_rpart, col = pal[4], add = TRUE)
plot(perf_ctree, col = pal[5], add = TRUE)
plot(perf_c50, col = pal[6], add = TRUE)
plot(perf_c50_boost, col = pal[7], add = TRUE)

# Adding the color legend
legend(x = "bottomright",
  legend = 
         c("logistic: sex + class", 
           "logistic: all",
           "logistic: stepwise",
           "rpart",
           "ctree",
           "c5.0",
           "c5.0 + boosting"),
       fill = pal)

```

From the plot we can see, that the question: "what model is the best?" has different answers depending on what sensitivity is required.

Let's consider the 3 models with largest AUC.


```{r}
plot(perf_lr_step, col = pal[3])
plot(perf_c50_boost, col = pal[7], add = TRUE)
plot(perf_ctree, col = pal[5], add = TRUE)

legend(x = "bottomright",
  legend = 
         c("logistic: stepwise",
           "c5.0 + boosting",
           "ctree"),
       fill = pal[c(3, 7, 5)])
```

The performance of the models is very similar for small FPR. Here we'd prefer a simpler logistic regression model. But if we would need to get sensitivity as high as possible while not sacrificing the FPR (0.3 < FPR < 0.7), the best model would be a much more complex ensemble of boosted trees produced by C5.0 algorithm. When the required sensitivity is very high (TPR > 0.9), there's again not much difference between the models.

In this example we could conclude, that all models show similar results, and additional factors should be used to select the best one. For example, if the interpretability is important, the logistic regression would be preferable. The ensemble of 20 trees, despite having slightly better results, is not interpretable at all.


Finally, let's consider a code example to plot all the ROCs simultaneously. This example is based on the fact, that `ROCR::performance()` function can compute all ROCs at once, when the predicted probabilities are assembled into a matrix using  `cbind()` function. 


```{r}
# Assembling the matrix with predicted positive class probabilities; a column name will be the plot label
preds <- cbind("logistic: stepwise" = pr_lr_step, 
               "c5.0 + boosting" = pr_c50_boost,
               "ctree" = pr_ctree)
# Assembling the matrix with actual class labels (it should have the same shape)
actuals <- matrix(d_test$survived, nrow = nrow(preds), ncol = ncol(preds))

# Computing data for ROCs
pred_mat <- ROCR::prediction(preds, actuals)
perf_mat <- ROCR::performance(pred_mat, "tpr", "fpr")

# Plotting all ROCs at once
plot(perf_mat, col = as.list(pal[c(3, 7, 5)])) # must convert palette vector to a list
legend("bottomright", legend = colnames(preds),
       fill = pal[c(3, 7, 5)])

```

